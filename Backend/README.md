# Resume Analyzer Backend API

A production-grade FastAPI backend for analyzing resumes against job descriptions using advanced ML techniques including BERT embeddings, TF-IDF similarity, RAG (Retrieval-Augmented Generation), and AI-powered feedback generation.

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Tech Stack](#tech-stack)
- [Project Structure](#project-structure)
- [Architecture](#architecture)
- [Setup & Installation](#setup--installation)
- [API Documentation](#api-documentation)
- [Environment Variables](#environment-variables)
- [Running the Application](#running-the-application)
- [Testing](#testing)
- [Performance Optimizations](#performance-optimizations)
- [Contributing](#contributing)

---

## ğŸ¯ Overview

This backend provides a comprehensive resume analysis system that:

- **Analyzes resumes** against job descriptions using multiple ML models
- **Generates AI feedback** using Groq LLM (Llama 3.3 70B)
- **Provides scoring** using hybrid approach (Skills + TF-IDF + BERT similarity)
- **Offers RAG-enhanced analysis** with FAISS vector search
- **Matches resumes** for HR dashboards
- **Performs market analysis** for job trends

### Key Highlights

- âœ… **Production-Ready**: Singleton database pattern, connection pooling, proper logging
- âœ… **Scalable**: ThreadPoolExecutor for non-blocking ML operations
- âœ… **High Performance**: Parallel processing, 20x faster batch operations
- âœ… **Enterprise-Grade**: Comprehensive error handling, graceful shutdown
- âœ… **AI-Powered**: LLM-based feedback generation with strict validation

---

## âœ¨ Features

### Core Features

1. **Resume Upload & Analysis**
   - PDF upload or Google Drive URL
   - Multi-model analysis (Skills, TF-IDF, BERT)
   - Hybrid scoring algorithm
   - RAG-enhanced context retrieval

2. **AI-Powered Feedback**
   - Groq LLM (Llama 3.3 70B) feedback generation
   - Strict field validation (CGPA, Degree, Branch, Experience)
   - Actionable improvement suggestions
   - Company-specific insights

3. **HR Dashboard**
   - Top matching resumes for job descriptions
   - Parallel processing for multiple resumes
   - Score-based ranking
   - Email-based deduplication

4. **Market Analysis**
   - Job market scraping
   - FAISS-based semantic search
   - LLM-powered market reports
   - Salary insights and skill demand analysis

5. **Authentication & Authorization**
   - JWT-based authentication
   - Secure password hashing (bcrypt)
   - Cookie-based sessions
   - User profile management

---

## ğŸ›  Tech Stack

### Backend Framework
- **FastAPI** - Modern, fast web framework
- **Uvicorn** - ASGI server

### Database
- **MongoDB** - NoSQL database with connection pooling

### ML & AI
- **spaCy** - NLP for text processing
- **SentenceTransformers** - BERT embeddings
- **scikit-learn** - TF-IDF vectorization
- **FAISS** - Vector similarity search
- **PyTorch** - Deep learning framework
- **Groq API** - LLM for feedback generation (Llama 3.3 70B)

### Utilities
- **PyMuPDF (fitz)** - PDF text extraction
- **Cloudinary** - File storage
- **BeautifulSoup4** - Web scraping
- **pydantic** - Data validation

### Authentication
- **python-jose** - JWT tokens
- **passlib** - Password hashing

---

## ğŸ“ Project Structure

```
Backend/
â”‚
â”œâ”€â”€ ğŸ“„ main.py                    # FastAPI app entry point, route registration
â”œâ”€â”€ ğŸ“„ app.py                     # (Legacy) Alternative app file
â”‚
â”œâ”€â”€ ğŸ—„ï¸ database.py                # Database singleton pattern with connection pooling
â”œâ”€â”€ ğŸ“Š ml_executor.py             # ThreadPoolExecutor for non-blocking ML operations
â”œâ”€â”€ ğŸ“ logging_config.py          # Centralized logging configuration
â”‚
â”œâ”€â”€ ğŸ” auth.py                    # Authentication endpoints (signup, login, logout)
â”œâ”€â”€ ğŸ” auth_utils.py              # Authentication utilities (JWT validation)
â”‚
â”œâ”€â”€ ğŸ“¤ uploads.py                 # Resume upload and analysis endpoints
â”œâ”€â”€ ğŸ§® calculation.py             # ML models (BERT, TF-IDF, RAG, scoring)
â”œâ”€â”€ ğŸ¤– ai_feedback.py             # LLM-powered feedback generation
â”‚
â”œâ”€â”€ ğŸ‘¥ hr_matches.py              # HR dashboard endpoints (top matching resumes)
â”œâ”€â”€ ğŸ“Š getData.py                 # User data retrieval endpoints
â”‚
â”œâ”€â”€ ğŸ“‹ schemas.py                 # Pydantic models (UserCreate, UserLogin, UserOut)
â”œâ”€â”€ ğŸ“‹ resume_Schemas.py          # Resume-related schemas
â”‚
â”œâ”€â”€ ğŸ› ï¸ utils.py                   # Utility functions (JWT, Cloudinary upload)
â”œâ”€â”€ ğŸ› ï¸ setup_env.py               # Environment setup helper
â”‚
â”œâ”€â”€ ğŸ“Š market_analysis/           # Market analysis module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ router.py                 # Market analysis API endpoints
â”‚   â”œâ”€â”€ market_analyzer.py        # Main market analysis pipeline
â”‚   â”œâ”€â”€ ingest.py                 # Job data ingestion
â”‚   â”œâ”€â”€ serp_scraper.py           # Job posting scraper
â”‚   â”œâ”€â”€ data_processor.py         # Data cleaning and processing
â”‚   â”œâ”€â”€ indexer.py                # FAISS index building
â”‚   â”œâ”€â”€ retriever.py              # FAISS-based retrieval
â”‚   â”œâ”€â”€ rag_store.py              # RAG store implementation
â”‚   â””â”€â”€ llm_reporter.py           # LLM-based report generation
â”‚
â”œâ”€â”€ ğŸ§ª test_all_apis.py           # Comprehensive API testing
â”œâ”€â”€ ğŸ§ª test_auth.py               # Authentication tests
â”œâ”€â”€ ğŸ§ª test_db.py                 # Database connection tests
â”‚
â”œâ”€â”€ ğŸ“ˆ locustfile.py              # Load testing configuration
â”‚
â”œâ”€â”€ ğŸ“¦ requirements.txt           # Python dependencies
â”œâ”€â”€ ğŸ³ Dockerfile                 # Docker configuration
â”‚
â”œâ”€â”€ ğŸ“š README.md                  # This file
â”œâ”€â”€ ğŸ“š SETUP.md                   # Setup instructions
â”œâ”€â”€ ğŸ“š IMPROVEMENTS.md            # Code improvements documentation
â”‚
â”œâ”€â”€ ğŸ“ logs/                      # Application logs (auto-generated)
â”‚   â””â”€â”€ app.log                   # Rotating log file
â”‚
â”œâ”€â”€ ğŸ“ market_data/               # Market analysis data storage
â”‚   â””â”€â”€ (FAISS indexes, metadata)
â”‚
â””â”€â”€ ğŸ“ __pycache__/               # Python bytecode cache
```

---

## ğŸ— Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend  â”‚
â”‚  (React)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ HTTP/REST
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         FastAPI Application         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   API Endpoints (Routers)     â”‚ â”‚
â”‚  â”‚  - /auth                      â”‚ â”‚
â”‚  â”‚  - /resume                    â”‚ â”‚
â”‚  â”‚  - /hr                        â”‚ â”‚
â”‚  â”‚  - /getme                     â”‚ â”‚
â”‚  â”‚  - /market                    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                 â”‚                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   ThreadPoolExecutor          â”‚ â”‚
â”‚  â”‚   (ML Operations)             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚             â”‚
    â–¼             â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MongoDB â”‚  â”‚ Groq    â”‚  â”‚Cloudinaryâ”‚
â”‚         â”‚  â”‚  LLM    â”‚  â”‚  (Files) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ML Pipeline Architecture

```
Resume PDF/URL
    â”‚
    â–¼
Text Extraction (PyMuPDF)
    â”‚
    â”œâ”€â”€â–º Skill Extraction (spaCy)
    â”œâ”€â”€â–º Field Extraction (Regex + spaCy)
    â”‚
    â–¼
Analysis Pipeline
    â”œâ”€â”€â–º Skill Matching Score
    â”œâ”€â”€â–º TF-IDF Similarity
    â”œâ”€â”€â–º BERT Embeddings (SentenceTransformer)
    â””â”€â”€â–º RAG Retrieval (FAISS)
    â”‚
    â–¼
Hybrid Score Calculation
    â”‚
    â–¼
AI Feedback Generation (Groq LLM)
    â”œâ”€â”€â–º RAG Context
    â”œâ”€â”€â–º Strict Validation
    â””â”€â”€â–º Actionable Feedback
    â”‚
    â–¼
Response to Frontend
```

### Database Architecture

```
MongoDB: resume_db
â”‚
â”œâ”€â”€ user_data (Collection)
â”‚   â”œâ”€â”€ _id
â”‚   â”œâ”€â”€ fullName
â”‚   â”œâ”€â”€ email
â”‚   â””â”€â”€ password (hashed)
â”‚
â””â”€â”€ resumes (Collection)
    â”œâ”€â”€ _id
    â”œâ”€â”€ email
    â”œâ”€â”€ resumeUrl
    â”œâ”€â”€ driveUrl
    â”œâ”€â”€ companyName
    â”œâ”€â”€ companyUrl
    â”œâ”€â”€ aiFeedback
    â”œâ”€â”€ scores
    â”‚   â”œâ”€â”€ skillScore
    â”‚   â”œâ”€â”€ tfidfScore
    â”‚   â”œâ”€â”€ bertScore
    â”‚   â””â”€â”€ hybridScore
    â”œâ”€â”€ ragData
    â””â”€â”€ uploadedAt
```

---

## ğŸš€ Setup & Installation

### Prerequisites

- Python 3.8+
- MongoDB (local or cloud)
- Node.js 16+ (for frontend)
- spaCy English model

### Step 1: Clone Repository

```bash
cd Resume/Backend
```

### Step 2: Create Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

### Step 4: Environment Configuration

Create a `.env` file in the Backend directory:

```env
# MongoDB Configuration
MONGODB_URI=mongodb://localhost:27017/resume_db
# Or MongoDB Atlas: mongodb+srv://user:password@cluster.mongodb.net/resume_db

# JWT Configuration
JWT_SECRET=your-secret-key-here-change-in-production
JWT_ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# Cloudinary Configuration (for file uploads)
CLOUDINARY_URL=cloudinary://api_key:api_secret@cloud_name

# Groq API (for LLM feedback)
GROQ_API_KEY=your-groq-api-key

# Optional: Market Analysis
EMBED_MODEL=sentence-transformers/all-mpnet-base-v2
MARKET_DATA_DIR=./market_analysis/market_data
```

### Step 5: Start MongoDB

**Local MongoDB:**
```bash
mongod  # or use systemd service
```

**MongoDB Atlas:** No local setup needed, use connection string in `.env`

### Step 6: Run the Application

```bash
# Development mode (with auto-reload)
uvicorn main:app --reload --host 0.0.0.0 --port 8000

# Production mode
uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4
```

The API will be available at: `http://localhost:8000`

### Step 7: Access API Documentation

- **Swagger UI**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

---

## ğŸ“š API Documentation

### Authentication Endpoints

#### POST `/auth/signup`
Register a new user.

**Request Body:**
```json
{
  "fullName": "John Doe",
  "email": "john@example.com",
  "password": "securepassword123"
}
```

**Response:**
```json
{
  "_id": "user_id",
  "fullName": "John Doe",
  "email": "john@example.com"
}
```

#### POST `/auth/login`
Login user and set JWT cookie.

**Request Body:**
```json
{
  "email": "john@example.com",
  "password": "securepassword123"
}
```

#### POST `/auth/logout`
Logout user (clears JWT cookie).

#### GET `/auth/check-auth`
Check if user is authenticated.

---

### Resume Analysis Endpoints

#### POST `/resume/upload-resume-analyze`
Upload and analyze a resume against a job description.

**Form Data:**
- `file`: PDF file (optional if `drive_url` provided)
- `jd_text`: Job description text (required)
- `drive_url`: Google Drive URL (optional)
- `company_name`: Company name (optional)
- `company_url`: Company website URL (optional)

**Response:**
```json
{
  "message": "Resume uploaded and analyzed successfully",
  "resumeId": "resume_id",
  "resumeUrl": "cloudinary_url",
  "driveUrl": "drive_url",
  "companyName": "Company Name",
  "companyUrl": "https://company.com",
  "resumeText": "extracted text...",
  "resumeSkills": ["Python", "JavaScript", ...],
  "jdSkills": ["Python", "React", ...],
  "matchedSkills": ["Python", ...],
  "missingSkills": ["React", ...],
  "skillScore": 75.5,
  "tfidfScore": 68.2,
  "bertScore": 82.1,
  "hybridScore": 76.8,
  "aiFeedback": {
    "feedback": ["feedback point 1", "feedback point 2", ...],
    "overallScore": 76.8,
    "feedbackType": "LLM-Powered (RAG Enhanced)",
    "strictValidation": {...},
    "hasCriticalIssues": false
  },
  "ragData": {
    "topChunks": [...],
    "companyInfo": "...",
    "ragEnabled": true
  }
}
```

#### POST `/resume/guest-analyze`
Analyze a sample resume (no authentication required).

---

### HR Dashboard Endpoints

#### POST `/hr/top-matches`
Get top matching resumes for a job description.

**Form Data:**
- `jd_text`: Job description text (required)

**Response:**
```json
{
  "message": "Top matching resumes retrieved",
  "jd": "job description text...",
  "count": 10,
  "topResumes": [
    {
      "resumeId": "id",
      "email": "user@example.com",
      "resumeUrl": "url",
      "driveUrl": "drive_url",
      "matchedSkills": ["Python", ...],
      "scores": {
        "skillScore": 85.0,
        "tfidfScore": 78.5,
        "bertScore": 88.2,
        "hybridScore": 84.5
      }
    },
    ...
  ]
}
```

---

### User Data Endpoints

#### GET `/getme/resumes`
Get all resumes for the authenticated user.

**Response:**
```json
{
  "email": "user@example.com",
  "count": 3,
  "resumes": [
    {
      "id": "resume_id",
      "email": "user@example.com",
      "resumeUrl": "url",
      "driveUrl": "drive_url",
      "aiFeedback": {...},
      "scores": {...},
      "uploadedAt": "2024-01-15T10:30:00"
    },
    ...
  ]
}
```

#### GET `/getme/?email=user@example.com`
Get resumes by email (query parameter).

---

### Market Analysis Endpoints

#### POST `/market/quick-analyze`
Perform market analysis for a job query.

**Request Body:**
```json
{
  "query": "Python developer",
  "location": "United States",
  "top_k": 5,
  "use_llm": true
}
```

**Response:**
```json
{
  "success": true,
  "query": "Python developer",
  "location": "United States",
  "skills_analyzed": ["Python", "JavaScript", ...],
  "total_results": 20,
  "hits": [...],
  "report": {
    "demand_level": "HIGH",
    "demand_reason": [...],
    "top_skills": [...],
    "salary_insights": {...},
    "recommendations": "..."
  }
}
```

#### GET `/market/health`
Health check for market analysis service.

---

### Utility Endpoints

#### GET `/`
Root endpoint - API status.

**Response:**
```json
{
  "message": "Resume Analyzer Backend is running!"
}
```

#### GET `/health`
Health check endpoint.

**Response:**
```json
{
  "status": "healthy",
  "message": "Backend is operational",
  "database": "connected",
  "jwt_secret_configured": true
}
```

---

## ğŸ”§ Environment Variables

| Variable | Description | Required | Default |
|----------|-------------|----------|---------|
| `MONGODB_URI` | MongoDB connection string | âœ… Yes | - |
| `JWT_SECRET` | Secret key for JWT tokens | âœ… Yes | - |
| `JWT_ALGORITHM` | JWT algorithm | âŒ No | `HS256` |
| `ACCESS_TOKEN_EXPIRE_MINUTES` | Token expiration time | âŒ No | `30` |
| `CLOUDINARY_URL` | Cloudinary credentials | âœ… Yes | - |
| `GROQ_API_KEY` | Groq API key for LLM | âœ… Yes | - |
| `EMBED_MODEL` | Embedding model name | âŒ No | `all-MiniLM-L6-v2` |
| `MARKET_DATA_DIR` | Market data directory | âŒ No | `./market_analysis/market_data` |

---

## ğŸƒ Running the Application

### Development Mode

```bash
# Activate virtual environment
source venv/bin/activate

# Run with auto-reload
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Production Mode

```bash
# Run with multiple workers
uvicorn main:app --host 0.0.0.0 --port 8000 --workers 4

# Or use Gunicorn
gunicorn main:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000
```

### Docker

```bash
# Build image
docker build -t resume-analyzer-backend .

# Run container
docker run -p 8000:8000 --env-file .env resume-analyzer-backend
```

---

## ğŸ§ª Testing

### Run All Tests

```bash
python test_all_apis.py
```

### Test Authentication

```bash
python test_auth.py
```

### Test Database Connection

```bash
python test_db.py
```

### Load Testing (Locust)

```bash
locust -f locustfile.py --host=http://localhost:8000
```

Access Locust UI at: http://localhost:8089

---

## âš¡ Performance Optimizations

### 1. Database Singleton Pattern
- **Connection Pooling**: 50 max connections, 10 min connections
- **Automatic Retry**: Retries on network errors
- **Connection Reuse**: Efficient connection management

### 2. ThreadPoolExecutor for ML Operations
- **Non-Blocking**: ML work runs in background threads
- **Parallel Processing**: 4 workers for concurrent ML operations
- **Event Loop Freedom**: FastAPI stays responsive during ML processing

### 3. Parallel Resume Processing
- **HR Matches**: Processes multiple resumes simultaneously
- **20x Faster**: Batch operations are significantly faster
- **Scalable**: Handles enterprise-scale resume databases

### 4. Logging & Monitoring
- **Rotating Logs**: 10MB max, 5 backups
- **Structured Logging**: Easy to parse and analyze
- **Production-Ready**: Comprehensive error tracking

See [IMPROVEMENTS.md](./IMPROVEMENTS.md) for detailed performance improvements.

---

## ğŸ“Š Key Algorithms

### Hybrid Scoring

The final score is calculated using a weighted combination:

```
hybridScore = 0.5 Ã— skillScore + 0.2 Ã— tfidfScore + 0.3 Ã— bertScore
```

- **skillScore**: Percentage of JD skills found in resume
- **tfidfScore**: TF-IDF cosine similarity (0-100)
- **bertScore**: BERT embedding cosine similarity (0-100)

### RAG (Retrieval-Augmented Generation)

1. **Chunking**: Resume text split into 500-character chunks
2. **Embedding**: BERT embeddings generated for each chunk
3. **Indexing**: FAISS index built from embeddings
4. **Retrieval**: Top-k relevant chunks retrieved based on JD similarity
5. **Context**: Retrieved chunks used as context for LLM feedback

---

## ğŸ”’ Security Features

- âœ… **JWT Authentication**: Secure token-based authentication
- âœ… **Password Hashing**: bcrypt with salt
- âœ… **HTTPS Ready**: CORS configured for secure origins
- âœ… **Input Validation**: Pydantic models for data validation
- âœ… **SQL Injection Safe**: MongoDB (NoSQL) prevents SQL injection
- âœ… **Environment Variables**: Sensitive data not hardcoded

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“ License

This project is part of the Resume Analyzer application.

---

## ğŸ‘¥ Authors

- Backend Development Team
- ML/AI Integration Team

---

## ğŸ“ Support

For issues, questions, or contributions, please open an issue on the repository.

---

## ğŸ¯ Future Enhancements

- [ ] Real-time WebSocket updates for analysis progress
- [ ] Support for more file formats (DOCX, TXT)
- [ ] Advanced resume parsing (structured data extraction)
- [ ] Integration with LinkedIn API
- [ ] Resume versioning and comparison
- [ ] Multi-language support
- [ ] GraphQL API option
- [ ] Kubernetes deployment configurations

---

## ğŸ“š Additional Documentation

- [SETUP.md](./SETUP.md) - Detailed setup instructions
- [IMPROVEMENTS.md](./IMPROVEMENTS.md) - Code improvements and architecture decisions

---

**Last Updated**: December 2024  
**Version**: 1.0.0  
**Status**: Production Ready âœ…
